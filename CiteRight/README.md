# CiteRight ğŸ¯

A multi-source RAG (Retrieval-Augmented Generation) assistant that pulls content from **Wikipedia, StackExchange, arXiv, and Wikidata** for accurate, well-cited answers.

## âœ¨ Key Features

- **Multi-Source Intelligence**: Pulls from 4 authoritative sources
- **Fresh Data Every Time**: No stale cached results
- **Citation Diversity**: Maximum 2 citations per source for balanced answers
- **Quality Evaluation**: Optional metrics (faithfulness, accuracy, precision)
- **PDF Upload**: Add your own documents
- **Local & Private**: Everything runs on your machine with Ollama

## ğŸš€ Quick Start (3 Steps)

### Step 1: Install Ollama

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull the model
ollama pull wizardlm2:latest
```

### Step 2: Install Dependencies

```bash
# Create virtual environment (optional but recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install requirements
pip install -r requirements.txt
```

### Step 3: Start the Application

**Terminal 1** - Start Ollama:
```bash
ollama serve
```

**Terminal 2** - Start the API:
```bash
source venv/bin/activate  # If using venv
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

**Terminal 3** - Start the UI:
```bash
source venv/bin/activate  # If using venv
streamlit run ui/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
```

### Step 4: Open Your Browser

- **UI**: http://localhost:8501
- **API Docs**: http://localhost:8000/docs

## ğŸ“š How to Use

### Basic Query

1. Open http://localhost:8501
2. Type your question (e.g., "What is machine learning?")
3. Select sources (Wikipedia, StackExchange, arXiv, Wikidata)
4. Adjust "Max items per source" (default: 3)
5. Click **Search** ğŸ”

### Enable Quality Evaluation (Optional)

- Check â˜‘ï¸ "Enable Query Evaluation" 
- Takes longer but shows:
  - **Faithfulness Score**: How grounded the answer is
  - **Citation Accuracy**: Correctness of attributions
  - **Precision@k**: Relevance of retrieved content

### Upload Your Own PDFs

1. Click **"Upload PDF"** in the sidebar
2. Choose your PDF file
3. Click **"Upload PDF"** button
4. Your document is now searchable!

## ğŸ¯ Example Queries

```
"What is quantum mechanics?"
"Explain lemmatization in NLP"
"How does machine learning work?"
"What are the key differences between Python and JavaScript?"
```

## ğŸ”§ Configuration

Edit `.env` or `env.example` to customize:

```env
# Models
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
OLLAMA_MODEL=wizardlm2:latest

# Retrieval Settings
RETRIEVE_K=20          # Initial candidates
RERANK_TOP_K=5         # After reranking
CONTEXT_TOP_K=4        # Used in prompt
```

## ğŸ“Š Data Sources

| Source | Content | License | Max per Query |
|--------|---------|---------|---------------|
| **Wikipedia** | Encyclopedia | CC BY-SA 3.0 | 2 citations |
| **StackExchange** | Q&A | CC BY-SA 4.0 | 2 citations |
| **arXiv** | Research Papers | CC BY 4.0 | 2 citations |
| **Wikidata** | Structured Data | CC0 1.0 | 2 citations |

*Note: Citation diversity ensures balanced, multi-perspective answers*

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multi-Source Ingestion â”‚
â”‚  (Wikipedia, Stack,     â”‚
â”‚   arXiv, Wikidata)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hybrid Retrieval       â”‚
â”‚  (FAISS + BM25)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reranking              â”‚
â”‚  (Cross-Encoder)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source Diversification â”‚
â”‚  (Max 2 per source)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generation             â”‚
â”‚  (Ollama LLM)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Optional Evaluation    â”‚
â”‚  (Quality Metrics)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cited Answer           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Project Structure

```
CiteRight/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPI backend
â”‚   â”œâ”€â”€ models.py                  # Pydantic models
â”‚   â”œâ”€â”€ config.py                  # Settings
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ multiverse_ingester.py # Multi-source ingestion
â”‚       â”œâ”€â”€ wikipedia_ingester.py  # Wikipedia API
â”‚       â”œâ”€â”€ stackexchange_ingester.py # Stack API
â”‚       â”œâ”€â”€ arxiv_ingester.py      # arXiv API
â”‚       â”œâ”€â”€ wikidata_ingester.py   # Wikidata API
â”‚       â”œâ”€â”€ pdf_processor.py       # PDF handling
â”‚       â”œâ”€â”€ retriever.py           # Hybrid search
â”‚       â”œâ”€â”€ reranker.py            # Cross-encoder
â”‚       â”œâ”€â”€ generator.py           # Ollama LLM
â”‚       â”œâ”€â”€ evaluator.py           # Quality metrics
â”‚       â””â”€â”€ utils.py               # Helper functions
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py           # Web interface
â”œâ”€â”€ .cursor/
â”‚   â””â”€â”€ prompts.json               # System prompts
â”œâ”€â”€ requirements.txt               # Dependencies
â””â”€â”€ README.md                      # This file
```

## ğŸ” API Endpoints

### Query with Source Selection
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is machine learning?",
    "sources": ["wikipedia", "stackexchange"],
    "max_per_source": 3,
    "enable_evaluation": false
  }'
```

### Upload PDF
```bash
curl -X POST "http://localhost:8000/upload-pdf" \
  -F "file=@/path/to/your/document.pdf"
```

## âš™ï¸ Advanced Features

### 1. Citation Diversity
- Automatically limits to **2 citations per source**
- Ensures multi-perspective answers
- Prevents single-source dominance

### 2. Quality Evaluation
Enable to get:
- **Faithfulness Score**: 0-1 scale of answer grounding
- **Citation Accuracy**: Correctness of attributions  
- **Precision@k**: Relevance of retrieved chunks
- **Transparency Trace**: Sentence-level source mapping

### 3. Selective Re-ask
- Automatically refines low-confidence answers
- Stricter citation requirements on second pass
- Improves answer quality

## ğŸ› Troubleshooting

### Ollama not found
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If not, start it
ollama serve
```

### Model not found
```bash
# Pull the model
ollama pull wizardlm2:latest

# List available models
ollama list
```

### Port already in use
```bash
# Use different ports
uvicorn app.main:app --port 8001
streamlit run ui/streamlit_app.py --server.port 8502
```

### Slow responses
- Disable evaluation (faster)
- Reduce `max_per_source` (fewer API calls)
- Use a smaller Ollama model: `ollama pull llama3.2:1b`

## ğŸ“ Requirements

- **Python**: 3.9+
- **Ollama**: Latest version
- **RAM**: 8GB minimum (16GB recommended for wizardlm2)
- **Storage**: 5GB for model + embeddings

## ğŸ” Privacy

âœ… **100% Local Processing**
- All LLM inference happens on your machine
- No data sent to external LLM APIs
- Source content fetched from public APIs only

## ğŸ“„ License

MIT License - Feel free to use for your own projects!

## ğŸ¤ Contributing

Issues and pull requests welcome! This project demonstrates:
- Multi-source RAG architecture
- Hybrid retrieval systems
- Citation diversity mechanisms
- Quality evaluation frameworks
- Local LLM integration

## ğŸ™ Acknowledgments

Built with:
- [Ollama](https://ollama.ai) - Local LLM runtime
- [LangChain](https://langchain.com) - RAG framework
- [FAISS](https://faiss.ai) - Vector search
- [Sentence Transformers](https://www.sbert.net) - Embeddings
- [FastAPI](https://fastapi.tiangolo.com) - Backend API
- [Streamlit](https://streamlit.io) - Web UI

---

**Made with â¤ï¸ for accurate, well-cited AI responses**
